{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-02-21T20:15:06.586673Z","iopub.status.busy":"2023-02-21T20:15:06.586225Z","iopub.status.idle":"2023-02-21T20:15:25.342942Z","shell.execute_reply":"2023-02-21T20:15:25.341932Z","shell.execute_reply.started":"2023-02-21T20:15:06.586589Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: contractions in c:\\users\\ryanr\\anaconda3\\lib\\site-packages (0.1.72)\n","Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\ryanr\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n","Requirement already satisfied: pyahocorasick in c:\\users\\ryanr\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n","Requirement already satisfied: anyascii in c:\\users\\ryanr\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n"]},{"name":"stderr","output_type":"stream","text":["Keyring is skipped due to an exception: 'keyring.backends'\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\ryanr\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","# from sklearn.neural_network import MLPClassifier\n","# from sklearn.metrics import f1_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","# from sklearn.metrics.pairwise import euclidean_distances\n","import string\n","from sklearn.metrics.pairwise import cosine_similarity\n","!pip install contractions\n","import contractions\n","# from sklearn.preprocessing import StandardScaler\n","# from sklearn.pipeline import make_pipeline\n","import nltk\n","# from nltk.translate import meteor, meteor_score\n","nltk.download('wordnet')\n","# import Levenshtein\n","# from nltk.translate.bleu_score import sentence_bleu\n","# from nltk.tokenize import word_tokenize"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["df_a = pd.read_csv(\"data/articles.csv\")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["0    Garment Upper body\n","Name: product_group_name, dtype: object"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df_a[df_a['article_id'] == 108775015]['product_group_name']"]},{"cell_type":"code","execution_count":4,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2023-02-21T20:15:25.345403Z","iopub.status.busy":"2023-02-21T20:15:25.345064Z","iopub.status.idle":"2023-02-21T20:15:25.363487Z","shell.execute_reply":"2023-02-21T20:15:25.362064Z","shell.execute_reply.started":"2023-02-21T20:15:25.345375Z"},"trusted":true},"outputs":[],"source":["#Define new features\n","def new_features(col1, col2):\n","\n","    #Find Euclidean Distance\n","    edist = []\n","\n","    #Find cosine similarity\n","    csim = []\n","\n","    #Find lengths of sentences (words)\n","    length1 = []\n","    length2 = []\n","\n","    #Find slices of sentences (number of similar words between sentences)\n","    slices = []\n","\n","    #Find meteor scores\n","    mscores = []\n","\n","    #Find Levenshtein distance\n","    ldists = []\n","\n","    #Find BLEU scores\n","    bleu1 = []\n","    bleu2 = []\n","    bleu3 = []\n","\n","    #Find Levenshtein ratio\n","    lratios = []\n","\n","    vectorizer = TfidfVectorizer()\n","    for (sent1, sent2) in zip(col1, col2):\n","\n","        #Euclidean Distances\n","        corpus = [sent1, sent2]\n","        features = vectorizer.fit_transform(corpus).todense() \n","        edist.append(euclidean_distances(features[0], features[1])[0][0])\n","\n","        #Cosine Similarities\n","        vectors = vectorizer.fit_transform(corpus).toarray()\n","        csim.append(cosine_similarity(vectors)[0][1])\n","\n","        #Lengths of sentences (words)\n","        length1.append(len(sent1.split()))\n","        length2.append(len(sent2.split()))\n","\n","        #Slices\n","        slice = 0  \n","        for word1 in sent1.split():\n","            for word2 in sent2.split():\n","                if word1 == word2:\n","                    slice += 1\n","        slices.append(slice)\n","  \n","        #Meteor Scores\n","        mscore = meteor_score.meteor_score([word_tokenize(sent1)], word_tokenize(sent2))\n","        mscores.append(mscore)\n","\n","        #Levenshtein Distances\n","        ldist = Levenshtein.distance(sent1, sent2)\n","        ldists.append(ldist)\n","\n","        #Levenshtein Ratios\n","        lratio = Levenshtein.ratio(sent1, sent2)\n","        lratios.append(lratio)\n","\n","        #BLEU Scores\n","        bscore1 = sentence_bleu([sent1.split()], sent2.split(), weights=[1])\n","        bleu1.append(bscore1)\n","\n","        bscore2 = sentence_bleu([sent1.split()], sent2.split(), weights=[1/2, 1/2])\n","        bleu2.append(bscore2)\n","\n","        bscore3 = sentence_bleu([sent1.split()], sent2.split(), weights=[1/3, 1/3, 1/3])\n","        bleu3.append(bscore3)\n","\n","    #Find absolute value of difference between lengths\n","    lengthdiff = abs(np.array(length2) - np.array(length1))\n","\n","    return edist, csim, length1, length2, lengthdiff, slices, mscores, ldists, lratios, bleu1, bleu2, bleu3\n","\n","#Create df with new features given two string arrays\n","def new_df(col1, col2):\n","    edist, csim, length1, length2, lengthdiff, slices, mscores, ldists, lratios, bleu1, bleu2, bleu3 = new_features(col1, col2) \n","\n","    return_df = pd.DataFrame()\n","    return_df['edist'] = edist\n","    return_df['csim'] = csim\n","    return_df['length1'] = length1\n","    return_df['length2'] = length2\n","    return_df['lengthdiff'] = lengthdiff\n","    return_df['slices'] = slices\n","    return_df['mscores'] = mscores\n","    return_df['ldists'] = ldists\n","    return_df['lratios'] = lratios\n","    return_df['bleu1'] = bleu1\n","    return_df['bleu2'] = bleu2\n","    return_df['bleu3'] = bleu3\n","\n","    return return_df\n","\n","# import warnings\n","\n","# warnings.filterwarnings(action='ignore')\n","\n","# new_df()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-02-21T20:15:25.365458Z","iopub.status.busy":"2023-02-21T20:15:25.365153Z","iopub.status.idle":"2023-02-21T20:15:25.381672Z","shell.execute_reply":"2023-02-21T20:15:25.380558Z","shell.execute_reply.started":"2023-02-21T20:15:25.365432Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["#Clean the strings to remove punctation, remove common words, and lowercase\n","def clean_string(text):\n","    text = text.replace('â€™', '\\'')\n","    text = contractions.fix(text)\n","    text = ''.join([word for word in text if word not in string.punctuation])\n","    text = text.lower()\n","    return text\n","\n","# gar_up_body = df_a[df_a['product_group_name'] == 'Garment Upper body'].dropna()\n","\n","# clean_desc = df_a[df_a['product_group_name'] == 'Garment Upper body'].copy().dropna()[['article_id']]\n","# clean_desc['clean_desc'] = list(map(clean_string, gar_up_body['detail_desc']))\n","# clean_desc"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-02-21T20:15:25.386464Z","iopub.status.busy":"2023-02-21T20:15:25.385698Z","iopub.status.idle":"2023-02-21T20:15:25.396209Z","shell.execute_reply":"2023-02-21T20:15:25.395326Z","shell.execute_reply.started":"2023-02-21T20:15:25.386410Z"},"trusted":true},"outputs":[],"source":["def get_cos_sim(product_group_name):\n","    pgn_articles = df_a[df_a['product_group_name'] == product_group_name].dropna()\n","    clean_desc = pgn_articles[['article_id']]\n","    clean_desc['clean_desc'] = list(map(clean_string, pgn_articles['detail_desc']))\n","    \n","    tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\n","    tfidf_matrix = tf.fit_transform(clean_desc['clean_desc'])\n","    cos_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n","    \n","    return cos_sim"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-02-21T20:15:25.397686Z","iopub.status.busy":"2023-02-21T20:15:25.397345Z","iopub.status.idle":"2023-02-21T20:15:25.425759Z","shell.execute_reply":"2023-02-21T20:15:25.424687Z","shell.execute_reply.started":"2023-02-21T20:15:25.397655Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Garment Upper body       42741\n","Garment Lower body       19812\n","Garment Full body        13292\n","Accessories              11158\n","Underwear                 5490\n","Shoes                     5283\n","Swimwear                  3127\n","Socks & Tights            2442\n","Nightwear                 1899\n","Unknown                    121\n","Underwear/nightwear         54\n","Cosmetic                    49\n","Bags                        25\n","Items                       17\n","Furniture                   13\n","Garment and Shoe care        9\n","Stationery                   5\n","Interior textile             3\n","Fun                          2\n","Name: product_group_name, dtype: int64"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df_a['product_group_name'].value_counts()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-02-21T20:15:25.427273Z","iopub.status.busy":"2023-02-21T20:15:25.426961Z","iopub.status.idle":"2023-02-21T20:16:43.635587Z","shell.execute_reply":"2023-02-21T20:16:43.634205Z","shell.execute_reply.started":"2023-02-21T20:15:25.427246Z"},"trusted":true},"outputs":[],"source":["# temp = get_cos_sim('Fun') # change product_group_name, save temp\n","# np.savetxt('fun_cos_sim.txt', temp, fmt='%1.6f') # save cos_sim matrix as .txt file"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-02-21T18:16:16.448737Z","iopub.status.busy":"2023-02-21T18:16:16.448207Z","iopub.status.idle":"2023-02-21T18:16:16.463537Z","shell.execute_reply":"2023-02-21T18:16:16.461525Z","shell.execute_reply.started":"2023-02-21T18:16:16.448697Z"},"trusted":true},"outputs":[],"source":["# clean_desc.set_index('article_id', inplace=True)\n","# name = pd.Series(clean_desc.index)\n","# name"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-02-21T18:16:21.609059Z","iopub.status.busy":"2023-02-21T18:16:21.608435Z","iopub.status.idle":"2023-02-21T18:16:21.618815Z","shell.execute_reply":"2023-02-21T18:16:21.617478Z","shell.execute_reply.started":"2023-02-21T18:16:21.609007Z"},"trusted":true},"outputs":[],"source":["# input article_id and appropriate cos_sim matrix, get top 10 recommendations\n","def recommendations(article_id, cos_sim):\n","    pgn = df_a[df_a['article_id'] == article_id].reset_index()['product_group_name'][0]\n","\n","    pgn_articles = df_a[df_a['product_group_name'] == pgn].dropna()\n","    clean_desc = pgn_articles[['article_id']]\n","    clean_desc['clean_desc'] = list(map(clean_string, pgn_articles['detail_desc']))\n","    \n","    clean_desc.set_index('article_id', inplace=True)\n","    name = pd.Series(clean_desc.index)\n","    \n","    recommended_article = []\n","    idx = name[name == article_id].index[0]\n","    score_series = pd.Series(cos_sim[idx]).sort_values(ascending = False)\n","    top_10_indexes = list(score_series.iloc[1:11].index)\n","    for i in top_10_indexes:\n","        recommended_article.append(list(clean_desc.index)[i])\n","    return recommended_article"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["array([[1.      , 0.00191 , 0.014937, ..., 0.002238, 0.01441 , 0.010219],\n","       [0.00191 , 1.      , 0.026466, ..., 0.004008, 0.005439, 0.005437],\n","       [0.014937, 0.026466, 1.      , ..., 0.004262, 0.040018, 0.016975],\n","       ...,\n","       [0.002238, 0.004008, 0.004262, ..., 1.      , 0.003592, 0.013798],\n","       [0.01441 , 0.005439, 0.040018, ..., 0.003592, 1.      , 0.016687],\n","       [0.010219, 0.005437, 0.016975, ..., 0.013798, 0.016687, 1.      ]])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["temp = np.loadtxt('data/gfb_cos_sim.txt', dtype=float)\n","temp"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-02-21T18:19:23.380929Z","iopub.status.busy":"2023-02-21T18:19:23.380402Z","iopub.status.idle":"2023-02-21T18:19:23.444031Z","shell.execute_reply":"2023-02-21T18:19:23.442541Z","shell.execute_reply.started":"2023-02-21T18:19:23.380888Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\ryanr\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  import sys\n"]}],"source":["temp_pgn = df_a[df_a['product_group_name'] == 'Garment Full body'].dropna()['article_id']\n","\n","recs_df = pd.DataFrame(columns=['article_id', 'recommendations'])\n","for aid in temp_pgn:\n","    recs_df.loc[len(recs_df.index)] = [aid, recommendations(aid, temp)]"]},{"cell_type":"code","execution_count":13,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["recs_df.to_csv('gfb_recs.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# vectorizer = TfidfVectorizer()\n","\n","# import itertools\n","# lst1 = vectorizer.fit_transform(clean_desc).toarray()\n","# lst2 = vectorizer.fit_transform(clean_desc).toarray()\n","# lst3 = list(map(lambda x: np.dot(x[0], x[1])/(np.linalg.norm(x[0])*np.linalg.norm(x[1])) , itertools.product(lst1,lst2)))\n","# lst3"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_a[df_a['product_group_name'] == 'Garment Upper body']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_a['product_group_name']"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"vscode":{"interpreter":{"hash":"51060068504f0ac08f07c76779ded23f779c75b6ec98a50a3896701dbf655d52"}}},"nbformat":4,"nbformat_minor":4}
